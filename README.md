# Deep Learning Networks from scratch
The idea here is to understand and write each neural network from scratch and replace them with more state of the art activation functions such as swish or mish.

Resnet:

    - [x] Resnet50
    - [ ] Resnet101
    - [ ] Resnet152

| Activation Function | Matthews Correlation Coefficient |
|---------------------|----------------------------------|
| Relu                | 96.09                            |
| Swish               | 98.04                            |
| Mish                | 98.24                            | 